---
title: "IMDB Scoring"
author: "Peters"
date: "8/31/2020"
output: html_document
---

This analysis was done as a class for my "Experimental Learning through Kaggle" class at BYU. This is a warning that much of this code is not my own, but each section will be contain a comment at the top if it was completed by someone else, if there is no indication, it was work of my own

Data avaiable through kaggle at: https://www.kaggle.com/c/train495
Proffesors Analysis: 

```{r setup, include=FALSE}
library("tidyverse")
library(DataExplorer)
```

```{r}
test <- read.csv("imdb495/imdbTest.csv")
train <- read.csv("imdb495/imdbTrain.csv")


#we want to merge our test and training so that when we do out imputations are consistent
names(test)[names(test) == "Id"] <- "movie_title"
(train <- bind_rows(train = train, test = test, .id="Set"))
```

Identifying some summary statistics and identifying missinf values in our data set.

```{r}
summary(train)

plot_correlation(train, type="continuous", 
                 cor_args=list(use="pairwise.complete.obs"))
```

### Data Cleaning

First, we want to clean out data. This will allow us to look through our variables, be able to visualize all of them and get an idea if which variables will be useful or not. Variables were split up between groups. My group was only given the **"plot_keywords"** since it was going to require a little more work then the others, which is found here first. All other data cleaning was done by toher classmates or the professor, which can be found later on.


##### Plot Keywords

```{r}
sum(is.na(train$plot_keywords)) #Number of missing plot keywords

train$plot_keywords[1] #Example output so we know what were working with.

```

So, my group and I had many different ideas in how to handle this variable. But the first thing we needed to do identify how we would handle the strings as shown above. It was decided that we would not only split the strings by the "|", but also by spaces, and identify the most used (non-common) words and keep the top 100 words (since there were about 5000 words in total).


```{r}
#Finds top keywords in dataframe
keywordDF <- as.data.frame((train$plot_keywords %>% sapply(function(x) unlist(str_split(x, "[\\| ]"))) %>% unlist %>% unname %>% table %>% sort.int(decreasing = TRUE)))

#REmoves common words and takes top 100 words
keywordDF <- keywordDF[-which(keywordDF$. %in% stopwords::stopwords("en")), ] %>% head(100)

head(keywordDF)
```

Now that we know some of the most common words found in the key words, we needed to decide where to go from there. Some of teh ideas we came across were:
* Create dummy variables for each of the topp 100 words, and give each variable some sort of score that would tell us which ones to keep.
* Count how many of each of the top 100 words appears in each row and create a frequency variable. Also score these and create categorical variables on a scale of "Low", "Medium", and "High."
* Create a continous variable between 0 and 1 that scores each movie on how siginicant their keywords are among the top 100.

I decided to try frequency and see how that turn out.

```{r}
#creates empty list
sumList = rep(0, length(keywordDF))

#retreaves list of every keyword by row
temp <- train$plot_keywords %>% sapply(function(x) unlist(str_split(x, "[\\| ]"))) %>% unname

#compares every row and counts number of top key words in that row
for (i in 1:length(temp)){
  sumList[i] <- sum(unique(temp[[i]]) %in% keywordDF$.)
}

#sets list into dataframe
train$num_Key_words <- sumList

#This will create categorical variables based on frequency, these limits are arbitrary
train$keyword_cat <- ifelse(train$num_Key_words < 2, "Low", "Medium")
train$keyword_cat <- ifelse(train$num_Key_words >= 4,"high", train$keyword_cat)

```

Here we will visualize our new variables
```{r}
ggplot(data=train, mapping=aes(x=keyword_cat, y=imdb_score)) +
  geom_boxplot()  +
  xlab("Number of Key Words") + ylab("IMDB Score") +
  ggtitle("Key Word Category vs Score") + theme_light()

ggplot(data=train, mapping=aes(x=num_Key_words)) +
  geom_histogram() +
  xlab("Number of Key Words") + ggtitle("Frequency of Key Words") + theme_light()

ggplot(data=train, mapping=aes(x=num_Key_words, y=imdb_score)) +
  geom_point() +
  xlab("Number of Key Words") + ylab("IMDB Score") +
  ggtitle("Frequency of Key Words vs Score") + theme_light()
```


```{r}
# This is just an extra, I wanted to learn how to make dummies for top 100 words

tempDF <- train$plot_keywords %>% sapply(function(x) unlist(str_split(x, "[\\| ]"))) %>% qdapTools::mtabulate()

tempDF[, colnames(tempDF) %in% keywordDF$. ]

#I dont want it taking up space in RAM though, so im deleting tempDF
rm(tempDF)
```


After we came aback as a group, we didn't find anything too significant. As you can see above, the distributions don't really tell us there would be any significant pull on our response variable. All our categories and distributions seem to be about the same, so we feel that this would not help us identify IMDB score.

Ultimately, we decided to drop this variable. It is what it is.

##### Rest of the variables
Here begins the rest of the data cleaning done by the rest of the members of the class and the professor.

###### Basic Cleaning (Mean, meadian, mode imputations)
```{r}
#Duration - only one missing so just look it up and fill it in
train[is.na(train$duration),]$duration <- 116

## Color - mode imputation and convert to 0/1
train <- train %>% 
  mutate(color=replace(color, is.na(color), "Color"))
train <- train %>%
  mutate(color = ifelse(color == "Color", 1, 0))

## Language - only five missing values so we replace them
missing_languages <- c("English", "None", "None", "None", "None")
train$language[is.na(train$language)] <- missing_languages


## num_user_for_reviews = mean imputation
train <- train %>%
  mutate(num_user_for_reviews=replace(num_user_for_reviews, is.na(num_user_for_reviews), 
                                      mean(num_user_for_reviews, na.rm=TRUE)))

## num_critic_for_reviews - Median imputation
train[is.na(train[["num_critic_for_reviews"]]), "num_critic_for_reviews"] <- median(x = train[["num_critic_for_reviews"]], na.rm = TRUE)


## Director - convert to number of movies made by director
director_movie_count <- train %>%
  group_by(director_name) %>%
  summarise(movies_made = n())

train <- train %>%
  left_join(director_movie_count) %>%
  select(-director_name)

```

###### Some of our categorical variables needing longer code
```{r}
## Content-rating - collapse GP --> PG and create "other"
## X --> NC-17, TV-?? --> TV, M-->PG13
train <- train %>%
  mutate(content_rating=fct_explicit_na(content_rating, na_level = "Unknown")) %>%
  mutate(content_rating=fct_collapse(content_rating, PG=c("GP", "PG"),
                                     NC17=c("X", "NC-17"),
                                     TV=c("TV-14", "TV-G", "TV-PG"),
                                     PG13=c("PG-13","M")))

## Genres - get the main genre and number of genres assigned
train <- train %>% mutate(main_genre=(str_split(genres, "\\|") %>%
                                      sapply(., FUN=function(x){x[1]})),
                        num_genre=(str_split(genres, "\\|") %>%
                                     sapply(., FUN=length)))
#Some genres only have 1 movie so create "other" category
#that contains all categories with less than 10 movies
other.cat <- train %>% group_by(main_genre) %>% 
  summarize(n=n()) %>% filter(n<10) %>% pull(main_genre)
train <- train %>%
  mutate(main_genre=fct_collapse(main_genre, Other=other.cat))


```

###### Linear Regression imputation for budget
```{r}
## linear regression for budget
budget.lm <- lm(sqrt(budget)~num_critic_for_reviews+duration+num_voted_users+
                  cast_total_facebook_likes+title_year+
                  movie_facebook_likes+main_genre, data=train)
budget.preds <- (predict(budget.lm, newdata=(train %>% filter(is.na(budget)))))^2
train <- train %>%
  mutate(budget=replace(budget, is.na(budget), budget.preds))
```

###### Stochastic Regression imputation for budget
```{r}
## stochastic regression for gross
gross.lm <- lm(sqrt(gross)~num_critic_for_reviews+duration+num_voted_users+
                  cast_total_facebook_likes+title_year+
                  movie_facebook_likes+main_genre+budget, data=train)

gross.preds <- (predict(gross.lm, newdata=(train %>% filter(is.na(gross))))+
                  rnorm(sum(is.na(train$gross)), 0, sigma(gross.lm)))^2

train <- train %>%
  mutate(gross=replace(gross, is.na(gross), gross.preds))



```

###### actor_name columns
```{r}
## we created num_top_actors which tell us how many “top” actors were in a movie. “Top” actors were actors who were in multiple movies. All 3 actor column were used to decide who was a top actor.
all.actors <- train %>% select(actor_1_name, actor_2_name, actor_3_name) %>% do.call(c, args=.)
actors.freq <- data.frame(actor=all.actors) %>% filter(!is.na(actor)) %>%
  group_by(actor) %>% summarize(n=n()) %>%
  arrange(desc(n))
top.actors <- actors.freq %>% filter(n>10) %>% pull(actor)
train <- train %>%
  mutate(num_top_actors=(ifelse(actor_1_name%in%top.actors, 1, 0) +
                           ifelse(actor_2_name%in%top.actors, 1, 0) +
                           ifelse(actor_3_name%in%top.actors, 1, 0)))
```

###### facebook_like columns
```{r}
## we made a column num_pop_actors for the total number of popular actors in a movie based off of their Facebook likes. With this, we decided to throw out cast_facebook likes as it seemed repetitive. It’s pretty mute since there’s also actor Facebook likes

actor.likes <- train %>% select(actor_1_facebook_likes, actor_2_facebook_likes, actor_3_facebook_likes) %>%
  do.call(c, args=.)
actors.likes <- data.frame(actor=all.actors, likes=actor.likes) %>%
  filter(!is.na(actor)) %>% group_by(actor) %>% summarize(likes=max(likes)) %>%
  arrange(desc(likes))
pop.actors <- actors.likes %>% filter(likes>quantile(likes, probs=0.99)) %>%
  pull(actor)
train <- train %>%
  mutate(num_pop_actors=(ifelse(actor_1_name%in%pop.actors, 1, 0) +
                           ifelse(actor_2_name%in%pop.actors, 1, 0) +
                           ifelse(actor_3_name%in%pop.actors, 1, 0)))
```


###### Throwing out everything else (including the plot keywords)
```{r}
train <- train %>% select(-cast_total_facebook_likes, -movie_imdb_link, -facenumber_in_poster, -plot_keywords, -country, -movie_facebook_likes, -director_facebook_likes, -actor_1_name, -actor_2_name, -actor_3_name, -actor_1_facebook_likes, -actor_2_facebook_likes, -actor_3_facebook_likes, -genres, -cast_total_facebook_likes)
```


removing things off of RAM
```{r}
rm(list=c("gross.lm", "budget.lm", "actors.freq", "keywordDF", "temp", "director_movie_count", "actor.likes", "i", "gross.preds", "top.actors", "sumList", "pop.actors", "other.cat", "missing_languages", "budget.preds", "all.actors", "actors.likes"))
```

#### Writing cleaned data to csv, just cause
```{r}
write_csv(x=train, path="./CleanedIMDBData.csv")
```


## Visualizations of our now clean data

```{r}
pairs(~imdb_score+num_critic_for_reviews+duration+gross+num_voted_users+num_user_for_reviews+budget+title_year+aspect_ratio, data = train)
```



```{r}
ggplot(data=train, mapping=aes(x=gross, y=imdb_score)) +
  geom_point()


with(train, cor(gross, imdb_score, use="complete.obs"))
```


```{r}
ggplot(data=train, mapping=aes(x=log(num_user_for_reviews), y=imdb_score, color = content_rating)) +
  geom_point()
with(train, cor(gross, imdb_score, use="complete.obs"))
```


